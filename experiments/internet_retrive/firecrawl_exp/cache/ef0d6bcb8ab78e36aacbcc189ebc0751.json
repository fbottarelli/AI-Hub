{"success": true, "status": "completed", "completed": 5, "total": 5, "creditsUsed": 5, "expiresAt": "2025-01-05T14:33:10.000Z", "data": [{"markdown": "[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev)\n\nv1\n\nSearch or ask...\n\nSearch...\n\nNavigation\n\nCrawl Endpoints\n\nCancel Crawl\n\n[Documentation](/introduction)\n[SDKs](/sdks/overview)\n[Learn](https://www.firecrawl.dev/blog/category/tutorials)\n[API Reference](/api-reference/introduction)\n\nDELETE\n\n/\n\ncrawl\n\n/\n\n{id}\n\nSend\n\nAuthorization\n\nAuthorization\n\nstring\n\n\\*\n\nBearer\n\nAuthorization\n\nRequired\n\nstring\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\nPath\n\nid\n\nstring\n\n\\*\n\nid\n\nRequired\n\nstring\n\nThe ID of the crawl job\n\n#### Authorizations\n\n[\u200b](#authorization-authorization)\n\nAuthorization\n\nstring\n\nheaderrequired\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n[\u200b](#parameter-id)\n\nid\n\nstring\n\nrequired\n\nThe ID of the crawl job\n\n#### Response\n\n200 - application/json\n\n[\u200b](#response-success)\n\nsuccess\n\nboolean\n\n[\u200b](#response-message)\n\nmessage\n\nstring\n\n[Get Crawl Status](/api-reference/endpoint/crawl-get)\n[Map](/api-reference/endpoint/map)", "metadata": {"title": "Firecrawl Docs", "favicon": "https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/_generated/favicon/favicon-32x32.png?v=3", "language": "en", "ogLocaleAlternate": [], "viewport": "width=device-width", "msapplication-config": "https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/_generated/favicon/browserconfig.xml?v=3", "apple-mobile-web-app-title": "Firecrawl Docs", "application-name": "Firecrawl Docs", "msapplication-TileColor": "#000", "theme-color": "#ffffff", "charset": "utf-8", "og:type": "website", "og:site_name": "Firecrawl Docs", "twitter:card": "summary_large_image", "og:title": "Firecrawl Docs", "twitter:title": "Firecrawl Docs", "og:image": "/images/og.png", "twitter:image": "/images/og.png", "og:description": "Firecrawl allows you turn websites to LLM-ready data in seconds.", "og:url": "/api-reference/endpoint/crawl-delete", "next-head-count": "24", "sourceURL": "https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete", "url": "https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete", "statusCode": 200}}, {"markdown": "[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev)\n\nv1\n\nSearch or ask...\n\nSearch...\n\nNavigation\n\nScrape Endpoints\n\nGet Batch Scrape Status\n\n[Documentation](/introduction)\n[SDKs](/sdks/overview)\n[Learn](https://www.firecrawl.dev/blog/category/tutorials)\n[API Reference](/api-reference/introduction)\n\nGET\n\n/\n\nbatch\n\n/\n\nscrape\n\n/\n\n{id}\n\nSend\n\nAuthorization\n\nAuthorization\n\nstring\n\n\\*\n\nBearer\n\nAuthorization\n\nRequired\n\nstring\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\nPath\n\nid\n\nstring\n\n\\*\n\nid\n\nRequired\n\nstring\n\nThe ID of the batch scrape job\n\n#### Authorizations\n\n[\u200b](#authorization-authorization)\n\nAuthorization\n\nstring\n\nheaderrequired\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Path Parameters\n\n[\u200b](#parameter-id)\n\nid\n\nstring\n\nrequired\n\nThe ID of the batch scrape job\n\n#### Response\n\n200 - application/json\n\n[\u200b](#response-status)\n\nstatus\n\nstring\n\nThe current status of the batch scrape. Can be `scraping`, `completed`, or `failed`.\n\n[\u200b](#response-total)\n\ntotal\n\ninteger\n\nThe total number of pages that were attempted to be scraped.\n\n[\u200b](#response-completed)\n\ncompleted\n\ninteger\n\nThe number of pages that have been successfully scraped.\n\n[\u200b](#response-credits-used)\n\ncreditsUsed\n\ninteger\n\nThe number of credits used for the batch scrape.\n\n[\u200b](#response-expires-at)\n\nexpiresAt\n\nstring\n\nThe date and time when the batch scrape will expire.\n\n[\u200b](#response-next)\n\nnext\n\nstring | null\n\nThe URL to retrieve the next 10MB of data. Returned if the batch scrape is not completed or if the response is larger than 10MB.\n\n[\u200b](#response-data)\n\ndata\n\nobject\\[\\]\n\nThe data of the batch scrape.\n\nShow child attributes\n\n[\u200b](#response-data-markdown)\n\ndata.markdown\n\nstring\n\n[\u200b](#response-data-html)\n\ndata.html\n\nstring | null\n\nHTML version of the content on page if `includeHtml` is true\n\n[\u200b](#response-data-raw-html)\n\ndata.rawHtml\n\nstring | null\n\nRaw HTML content of the page if `includeRawHtml` is true\n\n[\u200b](#response-data-links)\n\ndata.links\n\nstring\\[\\]\n\nList of links on the page if `includeLinks` is true\n\n[\u200b](#response-data-screenshot)\n\ndata.screenshot\n\nstring | null\n\nScreenshot of the page if `includeScreenshot` is true\n\n[\u200b](#response-data-metadata)\n\ndata.metadata\n\nobject\n\nShow child attributes\n\n[\u200b](#response-data-metadata-title)\n\ndata.metadata.title\n\nstring\n\n[\u200b](#response-data-metadata-description)\n\ndata.metadata.description\n\nstring\n\n[\u200b](#response-data-metadata-language)\n\ndata.metadata.language\n\nstring | null\n\n[\u200b](#response-data-metadata-source-url)\n\ndata.metadata.sourceURL\n\nstring\n\n[\u200b](#response-data-metadata-any-other-metadata)\n\ndata.metadata.<any other metadata>\n\nstring\n\n[\u200b](#response-data-metadata-status-code)\n\ndata.metadata.statusCode\n\ninteger\n\nThe status code of the page\n\n[\u200b](#response-data-metadata-error)\n\ndata.metadata.error\n\nstring | null\n\nThe error message of the page\n\n[Batch Scrape](/api-reference/endpoint/batch-scrape)\n[Crawl](/api-reference/endpoint/crawl-post)", "metadata": {"title": "Firecrawl Docs", "favicon": "https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/_generated/favicon/favicon-32x32.png?v=3", "language": "en", "ogLocaleAlternate": [], "viewport": "width=device-width", "msapplication-config": "https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/_generated/favicon/browserconfig.xml?v=3", "apple-mobile-web-app-title": "Firecrawl Docs", "application-name": "Firecrawl Docs", "msapplication-TileColor": "#000", "theme-color": "#ffffff", "charset": "utf-8", "og:type": "website", "og:site_name": "Firecrawl Docs", "twitter:card": "summary_large_image", "og:title": "Firecrawl Docs", "twitter:title": "Firecrawl Docs", "og:image": "/images/og.png", "twitter:image": "/images/og.png", "og:description": "Firecrawl allows you turn websites to LLM-ready data in seconds.", "og:url": "/api-reference/endpoint/batch-scrape-get", "next-head-count": "24", "sourceURL": "https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get", "url": "https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get", "statusCode": 200}}, {"markdown": "[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev)\n\nv1\n\nSearch or ask...\n\nSearch...\n\nNavigation\n\nScrape Endpoints\n\nBatch Scrape\n\n[Documentation](/introduction)\n[SDKs](/sdks/overview)\n[Learn](https://www.firecrawl.dev/blog/category/tutorials)\n[API Reference](/api-reference/introduction)\n\nPOST\n\n/\n\nbatch\n\n/\n\nscrape\n\nSend\n\nAuthorization\n\nAuthorization\n\nstring\n\n\\*\n\nBearer\n\nAuthorization\n\nRequired\n\nstring\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\nBody\n\nobject\n\n\\*\n\nurls\n\narray\n\nurls\n\narray\n\nwebhook\n\nstring\n\nstringobject\n\nwebhook\n\nstring\n\nThe URL to send the webhook to. This will trigger for batch scrape started (batch\\_scrape.started), every page scraped (batch\\_scrape.page) and when the batch scrape is completed (batch\\_scrape.completed or batch\\_scrape.failed). The response will be the same as the `/scrape` endpoint.\n\nformats\n\narray\n\nformats\n\narray\n\nFormats to include in the output.\n\nonlyMainContent\n\nboolean\n\nSelect option\n\nonlyMainContent\n\nboolean\n\nOnly return the main content of the page excluding headers, navs, footers, etc.\n\nincludeTags\n\narray\n\nincludeTags\n\narray\n\nTags to include in the output.\n\nexcludeTags\n\narray\n\nexcludeTags\n\narray\n\nTags to exclude from the output.\n\nheaders\n\nobject\n\nheaders\n\nobject\n\nHeaders to send with the request. Can be used to send cookies, user-agent, etc.\n\nwaitFor\n\ninteger\n\nwaitFor\n\ninteger\n\nSpecify a delay in milliseconds before fetching the content, allowing the page sufficient time to load.\n\nmobile\n\nboolean\n\nSelect option\n\nmobile\n\nboolean\n\nSet to true if you want to emulate scraping from a mobile device. Useful for testing responsive pages and taking mobile screenshots.\n\nskipTlsVerification\n\nboolean\n\nSelect option\n\nskipTlsVerification\n\nboolean\n\nSkip TLS certificate verification when making requests\n\ntimeout\n\ninteger\n\ntimeout\n\ninteger\n\nTimeout in milliseconds for the request\n\nextract\n\nobject\n\nextract\n\nobject\n\nExtract object\n\nschema\n\nobject\n\nschema\n\nobject\n\nThe schema to use for the extraction (Optional)\n\nsystemPrompt\n\nstring\n\nsystemPrompt\n\nstring\n\nThe system prompt to use for the extraction (Optional)\n\nprompt\n\nstring\n\nprompt\n\nstring\n\nThe prompt to use for the extraction without a schema (Optional)\n\nactions\n\narray\n\nactions\n\narray\n\nActions to perform on the page before grabbing the content\n\nlocation\n\nobject\n\nlocation\n\nobject\n\nLocation settings for the request. When specified, this will use an appropriate proxy if available and emulate the corresponding language and timezone settings. Defaults to 'US' if not specified.\n\ncountry\n\nstring\n\ncountry\n\nstring\n\nISO 3166-1 alpha-2 country code (e.g., 'US', 'AU', 'DE', 'JP')\n\nlanguages\n\narray\n\nlanguages\n\narray\n\nPreferred languages and locales for the request in order of priority. Defaults to the language of the specified location. See https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept-Language\n\nremoveBase64Images\n\nboolean\n\nSelect option\n\nremoveBase64Images\n\nboolean\n\nRemoves all base 64 images from the output, which may be overwhelmingly long. The image's alt text remains in the output, but the URL is replaced with a placeholder.\n\nignoreInvalidURLs\n\nboolean\n\nSelect option\n\nignoreInvalidURLs\n\nboolean\n\nIf invalid URLs are specified in the urls array, they will be ignored. Instead of them failing the entire request, a batch scrape using the remaining valid URLs will be created, and the invalid URLs will be returned in the invalidURLs field of the response.\n\n#### Authorizations\n\n[\u200b](#authorization-authorization)\n\nAuthorization\n\nstring\n\nheaderrequired\n\nBearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.\n\n#### Body\n\napplication/json\n\n[\u200b](#body-urls)\n\nurls\n\nstring\\[\\]\n\n[\u200b](#body-webhook)\n\nwebhook\n\nstringobject\n\nThe URL to send the webhook to. This will trigger for batch scrape started (batch\\_scrape.started), every page scraped (batch\\_scrape.page) and when the batch scrape is completed (batch\\_scrape.completed or batch\\_scrape.failed). The response will be the same as the `/scrape` endpoint.\n\n[\u200b](#body-formats)\n\nformats\n\nenum<string>\\[\\]\n\nFormats to include in the output.\n\nAvailable options:\n\n`markdown`,\n\n`html`,\n\n`rawHtml`,\n\n`links`,\n\n`screenshot`,\n\n`extract`,\n\n`screenshot@fullPage`\n\n[\u200b](#body-only-main-content)\n\nonlyMainContent\n\nboolean\n\ndefault: true\n\nOnly return the main content of the page excluding headers, navs, footers, etc.\n\n[\u200b](#body-include-tags)\n\nincludeTags\n\nstring\\[\\]\n\nTags to include in the output.\n\n[\u200b](#body-exclude-tags)\n\nexcludeTags\n\nstring\\[\\]\n\nTags to exclude from the output.\n\n[\u200b](#body-headers)\n\nheaders\n\nobject\n\nHeaders to send with the request. Can be used to send cookies, user-agent, etc.\n\n[\u200b](#body-wait-for)\n\nwaitFor\n\ninteger\n\ndefault: 0\n\nSpecify a delay in milliseconds before fetching the content, allowing the page sufficient time to load.\n\n[\u200b](#body-mobile)\n\nmobile\n\nboolean\n\ndefault: false\n\nSet to true if you want to emulate scraping from a mobile device. Useful for testing responsive pages and taking mobile screenshots.\n\n[\u200b](#body-skip-tls-verification)\n\nskipTlsVerification\n\nboolean\n\ndefault: false\n\nSkip TLS certificate verification when making requests\n\n[\u200b](#body-timeout)\n\ntimeout\n\ninteger\n\ndefault: 30000\n\nTimeout in milliseconds for the request\n\n[\u200b](#body-extract)\n\nextract\n\nobject\n\nExtract object\n\nShow child attributes\n\n[\u200b](#body-extract-schema)\n\nextract.schema\n\nobject\n\nThe schema to use for the extraction (Optional)\n\n[\u200b](#body-extract-system-prompt)\n\nextract.systemPrompt\n\nstring\n\nThe system prompt to use for the extraction (Optional)\n\n[\u200b](#body-extract-prompt)\n\nextract.prompt\n\nstring\n\nThe prompt to use for the extraction without a schema (Optional)\n\n[\u200b](#body-actions)\n\nactions\n\nobject\\[\\]\n\nActions to perform on the page before grabbing the content\n\n*   Wait\n*   Screenshot\n*   Click\n*   Write text\n*   Press a key\n*   Scroll\n*   Scrape\n*   Execute JavaScript\n\nShow child attributes\n\n[\u200b](#body-actions-type)\n\nactions.type\n\nenum<string>\n\nrequired\n\nWait for a specified amount of milliseconds\n\nAvailable options:\n\n`wait`\n\n[\u200b](#body-actions-milliseconds)\n\nactions.milliseconds\n\ninteger\n\nNumber of milliseconds to wait\n\nRequired range: `x > 1`\n\n[\u200b](#body-actions-selector)\n\nactions.selector\n\nstring\n\nQuery selector to find the element by\n\n[\u200b](#body-location)\n\nlocation\n\nobject\n\nLocation settings for the request. When specified, this will use an appropriate proxy if available and emulate the corresponding language and timezone settings. Defaults to 'US' if not specified.\n\nShow child attributes\n\n[\u200b](#body-location-country)\n\nlocation.country\n\nstring\n\ndefault: US\n\nISO 3166-1 alpha-2 country code (e.g., 'US', 'AU', 'DE', 'JP')\n\n[\u200b](#body-location-languages)\n\nlocation.languages\n\nstring\\[\\]\n\nPreferred languages and locales for the request in order of priority. Defaults to the language of the specified location. See https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept-Language\n\n[\u200b](#body-remove-base64-images)\n\nremoveBase64Images\n\nboolean\n\nRemoves all base 64 images from the output, which may be overwhelmingly long. The image's alt text remains in the output, but the URL is replaced with a placeholder.\n\n[\u200b](#body-ignore-invalid-urls)\n\nignoreInvalidURLs\n\nboolean\n\ndefault: false\n\nIf invalid URLs are specified in the urls array, they will be ignored. Instead of them failing the entire request, a batch scrape using the remaining valid URLs will be created, and the invalid URLs will be returned in the invalidURLs field of the response.\n\n#### Response\n\n200 - application/json\n\n[\u200b](#response-success)\n\nsuccess\n\nboolean\n\n[\u200b](#response-id)\n\nid\n\nstring\n\n[\u200b](#response-url)\n\nurl\n\nstring\n\n[\u200b](#response-invalid-urls)\n\ninvalidURLs\n\nstring\\[\\] | null\n\nIf ignoreInvalidURLs is true, this is an array containing the invalid URLs that were specified in the request. If there were no invalid URLs, this will be an empty array. If ignoreInvalidURLs is false, this field will be undefined.\n\n[Scrape](/api-reference/endpoint/scrape)\n[Get Batch Scrape Status](/api-reference/endpoint/batch-scrape-get)", "metadata": {"title": "Firecrawl Docs", "favicon": "https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/_generated/favicon/favicon-32x32.png?v=3", "language": "en", "ogLocaleAlternate": [], "viewport": "width=device-width", "msapplication-config": "https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/_generated/favicon/browserconfig.xml?v=3", "apple-mobile-web-app-title": "Firecrawl Docs", "application-name": "Firecrawl Docs", "msapplication-TileColor": "#000", "theme-color": "#ffffff", "charset": "utf-8", "og:type": "website", "og:site_name": "Firecrawl Docs", "twitter:card": "summary_large_image", "og:title": "Firecrawl Docs", "twitter:title": "Firecrawl Docs", "og:image": "/images/og.png", "twitter:image": "/images/og.png", "og:description": "Firecrawl allows you turn websites to LLM-ready data in seconds.", "og:url": "/api-reference/endpoint/batch-scrape", "next-head-count": "24", "sourceURL": "https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape", "url": "https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape", "statusCode": 200}}, {"markdown": "[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev)\n\nv1\n\nSearch or ask...\n\nSearch...\n\nNavigation\n\nGet Started\n\nAdvanced Scraping Guide\n\n[Documentation](/introduction)\n[SDKs](/sdks/overview)\n[Learn](https://www.firecrawl.dev/blog/category/tutorials)\n[API Reference](/api-reference/introduction)\n\nThis guide will walk you through the different endpoints of Firecrawl and how to use them fully with all its parameters.\n\n[\u200b](#basic-scraping-with-firecrawl-scrape)\n\nBasic scraping with Firecrawl (/scrape)\n-------------------------------------------------------------------------------------\n\nTo scrape a single page and get clean markdown content, you can use the `/scrape` endpoint.\n\n[\u200b](#scraping-pdfs)\n\nScraping PDFs\n------------------------------------\n\n**Firecrawl supports scraping PDFs by default.** You can use the `/scrape` endpoint to scrape a PDF link and get the text content of the PDF. You can disable this by setting `parsePDF` to `false`.\n\n[\u200b](#scrape-options)\n\nScrape Options\n--------------------------------------\n\nWhen using the `/scrape` endpoint, you can customize the scraping behavior with many parameters. Here are the available options:\n\n### \n\n[\u200b](#setting-the-content-formats-on-response-with-formats)\n\nSetting the content formats on response with `formats`\n\n*   **Type**: `array`\n*   **Enum**: `[\"markdown\", \"links\", \"html\", \"rawHtml\", \"screenshot\"]`\n*   **Description**: Specify the formats to include in the response. Options include:\n    *   `markdown`: Returns the scraped content in Markdown format.\n    *   `links`: Includes all hyperlinks found on the page.\n    *   `html`: Provides the content in HTML format.\n    *   `rawHtml`: Delivers the raw HTML content, without any processing.\n    *   `screenshot`: Includes a screenshot of the page as it appears in the browser.\n    *   `extract`: Extracts structured information from the page using the LLM.\n*   **Default**: `[\"markdown\"]`\n\n### \n\n[\u200b](#getting-the-full-page-content-as-markdown-with-onlymaincontent)\n\nGetting the full page content as markdown with `onlyMainContent`\n\n*   **Type**: `boolean`\n*   **Description**: By default, the scraper will only return the main content of the page, excluding headers, navigation bars, footers, etc. Set this to `false` to return the full page content.\n*   **Default**: `true`\n\n### \n\n[\u200b](#setting-the-tags-to-include-with-includetags)\n\nSetting the tags to include with `includeTags`\n\n*   **Type**: `array`\n*   **Description**: Specify the HTML tags, classes and ids to include in the response.\n*   **Default**: undefined\n\n### \n\n[\u200b](#setting-the-tags-to-exclude-with-excludetags)\n\nSetting the tags to exclude with `excludeTags`\n\n*   **Type**: `array`\n*   **Description**: Specify the HTML tags, classes and ids to exclude from the response.\n*   **Default**: undefined\n\n### \n\n[\u200b](#waiting-for-the-page-to-load-with-waitfor)\n\nWaiting for the page to load with `waitFor`\n\n*   **Type**: `integer`\n*   **Description**: To be used only as a last resort. Wait for a specified amount of milliseconds for the page to load before fetching content.\n*   **Default**: `0`\n\n### \n\n[\u200b](#setting-the-maximum-timeout)\n\nSetting the maximum `timeout`\n\n*   **Type**: `integer`\n*   **Description**: Set the maximum duration in milliseconds that the scraper will wait for the page to respond before aborting the operation.\n*   **Default**: `30000` (30 seconds)\n\n### \n\n[\u200b](#example-usage)\n\nExample Usage\n\n    curl -X POST https://api.firecrawl.dev/v1/scrape \\\n        -H '\n        Content-Type: application/json' \\\n        -H 'Authorization : Bearer YOUR_API_KEY' \\\n        -d '{\n          \"url\": \"https://docs.firecrawl.dev\",\n          \"formats\": [\"markdown\", \"links\", \"html\", \"rawHtml\", \"screenshot\"],\n          \"includeTags\": [\"h1\", \"p\", \"a\", \".main-content\"],\n          \"excludeTags\": [\"#ad\", \"#footer\"],\n          \"onlyMainContent\": false,\n          \"waitFor\": 1000,\n          \"timeout\": 15000\n        }'\n    \n\nIn this example, the scraper will:\n\n*   Return the full page content as markdown.\n*   Include the markdown, raw HTML, HTML, links and screenshot in the response.\n*   The response will include only the HTML tags `<h1>`, `<p>`, `<a>`, and elements with the class `.main-content`, while excluding any elements with the IDs `#ad` and `#footer`.\n*   Wait for 1000 milliseconds (1 second) for the page to load before fetching the content.\n*   Set the maximum duration of the scrape request to 15000 milliseconds (15 seconds).\n\nHere is the API Reference for it: [Scrape Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape)\n\n[\u200b](#extractor-options)\n\nExtractor Options\n--------------------------------------------\n\nWhen using the `/scrape` endpoint, you can specify options for **extracting structured information** from the page content using the `extract` parameter. Here are the available options:\n\n### \n\n[\u200b](#using-the-llm-extraction)\n\nUsing the LLM Extraction\n\n### \n\n[\u200b](#schema)\n\nschema\n\n*   **Type**: `object`\n*   **Required**: False if prompt is provided\n*   **Description**: The schema for the data to be extracted. This defines the structure of the extracted data.\n\n### \n\n[\u200b](#system-prompt)\n\nsystem prompt\n\n*   **Type**: `string`\n*   **Required**: False\n*   **Description**: System prompt for the LLM.\n\n### \n\n[\u200b](#prompt)\n\nprompt\n\n*   **Type**: `string`\n*   **Required**: False if schema is provided\n*   **Description**: A prompt for the LLM to extract the data in the correct structure.\n*   **Example**: `\"Extract the features of the product\"`\n\n### \n\n[\u200b](#example-usage-2)\n\nExample Usage\n\n    curl -X POST https://api.firecrawl.dev/v0/scrape \\\n        -H 'Content-Type: application/json' \\\n        -H 'Authorization: Bearer YOUR_API_KEY' \\\n        -d '{\n          \"url\": \"https://firecrawl.dev\",\n          \"formats\": [\"markdown\", \"extract\"],\n          \"extract\": {\n            \"prompt\": \"Extract the features of the product\"\n          }\n        }'\n    \n\n    {\n      \"success\": true,\n      \"data\": {\n        \"content\": \"Raw Content\",\n        \"metadata\": {\n          \"title\": \"Mendable\",\n          \"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n          \"robots\": \"follow, index\",\n          \"ogTitle\": \"Mendable\",\n          \"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n          \"ogUrl\": \"https://docs.firecrawl.dev/\",\n          \"ogImage\": \"https://docs.firecrawl.dev/mendable_new_og1.png\",\n          \"ogLocaleAlternate\": [],\n          \"ogSiteName\": \"Mendable\",\n          \"sourceURL\": \"https://docs.firecrawl.dev/\",\n          \"statusCode\": 200\n        },\n        \"extract\": {\n          \"product\": \"Firecrawl\",\n          \"features\": {\n            \"general\": {\n              \"description\": \"Turn websites into LLM-ready data.\",\n              \"openSource\": true,\n              \"freeCredits\": 500,\n              \"useCases\": [\\\n                \"AI applications\",\\\n                \"Data science\",\\\n                \"Market research\",\\\n                \"Content aggregation\"\\\n              ]\n            },\n            \"crawlingAndScraping\": {\n              \"crawlAllAccessiblePages\": true,\n              \"noSitemapRequired\": true,\n              \"dynamicContentHandling\": true,\n              \"dataCleanliness\": {\n                \"process\": \"Advanced algorithms\",\n                \"outputFormat\": \"Markdown\"\n              }\n            },\n            ...\n          }\n        }\n      }\n    }\n    \n\n[\u200b](#actions)\n\nActions\n------------------------\n\nWhen using the `/scrape` endpoint, Firecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.\n\n### \n\n[\u200b](#available-actions)\n\nAvailable Actions\n\n#### \n\n[\u200b](#wait)\n\nwait\n\n*   **Type**: `object`\n*   **Description**: Wait for a specified amount of milliseconds.\n*   **Properties**:\n    *   `type`: `\"wait\"`\n    *   `milliseconds`: Number of milliseconds to wait.\n*   **Example**:\n    \n        {\n          \"type\": \"wait\",\n          \"milliseconds\": 2000\n        }\n        \n    \n\n#### \n\n[\u200b](#screenshot)\n\nscreenshot\n\n*   **Type**: `object`\n*   **Description**: Take a screenshot.\n*   **Properties**:\n    *   `type`: `\"screenshot\"`\n    *   `fullPage`: Should the screenshot be full-page or viewport sized? (default: `false`)\n*   **Example**:\n    \n        {\n          \"type\": \"screenshot\",\n          \"fullPage\": true\n        }\n        \n    \n\n#### \n\n[\u200b](#click)\n\nclick\n\n*   **Type**: `object`\n*   **Description**: Click on an element.\n*   **Properties**:\n    *   `type`: `\"click\"`\n    *   `selector`: Query selector to find the element by.\n*   **Example**:\n    \n        {\n          \"type\": \"click\",\n          \"selector\": \"#load-more-button\"\n        }\n        \n    \n\n#### \n\n[\u200b](#write)\n\nwrite\n\n*   **Type**: `object`\n*   **Description**: Write text into an input field.\n*   **Properties**:\n    *   `type`: `\"write\"`\n    *   `text`: Text to type.\n    *   `selector`: Query selector for the input field.\n*   **Example**:\n    \n        {\n          \"type\": \"write\",\n          \"text\": \"Hello, world!\",\n          \"selector\": \"#search-input\"\n        }\n        \n    \n\n#### \n\n[\u200b](#press)\n\npress\n\n*   **Type**: `object`\n*   **Description**: Press a key on the page.\n*   **Properties**:\n    *   `type`: `\"press\"`\n    *   `key`: Key to press.\n*   **Example**:\n    \n        {\n          \"type\": \"press\",\n          \"key\": \"Enter\"\n        }\n        \n    \n\n#### \n\n[\u200b](#scroll)\n\nscroll\n\n*   **Type**: `object`\n*   **Description**: Scroll the page.\n*   **Properties**:\n    *   `type`: `\"scroll\"`\n    *   `direction`: Direction to scroll (`\"up\"` or `\"down\"`).\n    *   `amount`: Amount to scroll in pixels.\n*   **Example**:\n    \n        {\n          \"type\": \"scroll\",\n          \"direction\": \"down\",\n          \"amount\": 500\n        }\n        \n    \n\nFor more details about the actions parameters, refer to the [API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape)\n.\n\n[\u200b](#crawling-multiple-pages)\n\nCrawling Multiple Pages\n--------------------------------------------------------\n\nTo crawl multiple pages, you can use the `/crawl` endpoint. This endpoint allows you to specify a base URL you want to crawl and all accessible subpages will be crawled.\n\n    curl -X POST https://api.firecrawl.dev/v1/crawl \\\n        -H 'Content-Type: application/json' \\\n        -H 'Authorization: Bearer YOUR_API_KEY' \\\n        -d '{\n          \"url\": \"https://docs.firecrawl.dev\"\n        }'\n    \n\nReturns a id\n\n    { \"id\": \"1234-5678-9101\" }\n    \n\n### \n\n[\u200b](#check-crawl-job)\n\nCheck Crawl Job\n\nUsed to check the status of a crawl job and get its result.\n\n    curl -X GET https://api.firecrawl.dev/v1/crawl/1234-5678-9101 \\\n      -H 'Content-Type: application/json' \\\n      -H 'Authorization: Bearer YOUR_API_KEY'\n    \n\n#### \n\n[\u200b](#pagination-next-url)\n\nPagination/Next URL\n\nIf the content is larger than 10MB or if the crawl job is still running, the response will include a `next` parameter. This parameter is a URL to the next page of results. You can use this parameter to get the next page of results.\n\n### \n\n[\u200b](#crawler-options)\n\nCrawler Options\n\nWhen using the `/crawl` endpoint, you can customize the crawling behavior with request body parameters. Here are the available options:\n\n#### \n\n[\u200b](#includepaths)\n\n`includePaths`\n\n*   **Type**: `array`\n*   **Description**: URL patterns to include in the crawl. Only URLs matching these patterns will be crawled.\n*   **Example**: `[\"/blog/*\", \"/products/*\"]`\n\n#### \n\n[\u200b](#excludepaths)\n\n`excludePaths`\n\n*   **Type**: `array`\n*   **Description**: URL patterns to exclude from the crawl. URLs matching these patterns will be skipped.\n*   **Example**: `[\"/admin/*\", \"/login/*\"]`\n\n#### \n\n[\u200b](#maxdepth)\n\n`maxDepth`\n\n*   **Type**: `integer`\n*   **Description**: Maximum depth to crawl relative to the entered URL. A maxDepth of 0 scrapes only the entered URL. A maxDepth of 1 scrapes the entered URL and all pages one level deep. A maxDepth of 2 scrapes the entered URL and all pages up to two levels deep. Higher values follow the same pattern.\n*   **Example**: `2`\n\n#### \n\n[\u200b](#limit)\n\n`limit`\n\n*   **Type**: `integer`\n*   **Description**: Maximum number of pages to crawl.\n*   **Default**: `10000`\n\n#### \n\n[\u200b](#allowbackwardlinks)\n\n`allowBackwardLinks`\n\n*   **Type**: `boolean`\n*   **Description**: This option permits the crawler to navigate to URLs that are higher in the directory structure than the base URL. For instance, if the base URL is `example.com/blog/topic`, enabling this option allows crawling to pages like `example.com/blog` or `example.com`, which are backward in the path hierarchy relative to the base URL.\n*   **Default**: `false`\n\n### \n\n[\u200b](#allowexternallinks)\n\n`allowExternalLinks`\n\n*   **Type**: `boolean`\n*   **Description**: This option allows the crawler to follow links that point to external domains. Be careful with this option, as it can cause the crawl to stop only based only on the`limit` and `maxDepth` values.\n*   **Default**: `false`\n\n#### \n\n[\u200b](#scrapeoptions)\n\nscrapeOptions\n\nAs part of the crawler options, you can also specify the `scrapeOptions` parameter. This parameter allows you to customize the scraping behavior for each page.\n\n*   **Type**: `object`\n*   **Description**: Options for the scraper.\n*   **Example**: `{\"formats\": [\"markdown\", \"links\", \"html\", \"rawHtml\", \"screenshot\"], \"includeTags\": [\"h1\", \"p\", \"a\", \".main-content\"], \"excludeTags\": [\"#ad\", \"#footer\"], \"onlyMainContent\": false, \"waitFor\": 1000, \"timeout\": 15000}`\n*   **Default**: `{ \"formats\": [\"markdown\"] }`\n*   **See**: [Scrape Options](/_sites/docs.firecrawl.dev/advanced-scraping-guide#setting-the-content-formats-on-response-with-formats)\n    \n\n### \n\n[\u200b](#example-usage-3)\n\nExample Usage\n\n    curl -X POST https://api.firecrawl.dev/v1/crawl \\\n        -H 'Content-Type: application/json' \\\n        -H 'Authorization : Bearer YOUR_API_KEY' \\\n        -d '{\n          \"url\": \"https://docs.firecrawl.dev\",\n          \"includePaths\": [\"/blog/*\", \"/products/*\"],\n          \"excludePaths\": [\"/admin/*\", \"/login/*\"],\n          \"maxDepth\": 2,\n          \"limit\": 1000\n        }'\n    \n\nIn this example, the crawler will:\n\n*   Only crawl URLs that match the patterns `/blog/*` and `/products/*`.\n*   Skip URLs that match the patterns `/admin/*` and `/login/*`.\n*   Return the full document data for each page.\n*   Crawl up to a maximum depth of 2.\n*   Crawl a maximum of 1000 pages.\n\n[\u200b](#mapping-website-links-with-map)\n\nMapping Website Links with `/map`\n-------------------------------------------------------------------------\n\nThe `/map` endpoint is adept at identifying URLs that are contextually related to a given website. This feature is crucial for understanding a site\u2019s contextual link environment, which can greatly aid in strategic site analysis and navigation planning.\n\n### \n\n[\u200b](#usage)\n\nUsage\n\nTo use the `/map` endpoint, you need to send a GET request with the URL of the page you want to map. Here is an example using `curl`:\n\n    curl -X POST https://api.firecrawl.dev/v1/map \\\n        -H 'Content-Type: application/json' \\\n        -H 'Authorization: Bearer YOUR_API_KEY' \\\n        -d '{\n          \"url\": \"https://docs.firecrawl.dev\"\n        }'\n    \n\nThis will return a JSON object containing links contextually related to the url.\n\n### \n\n[\u200b](#example-response)\n\nExample Response\n\n      {\n        \"success\":true,\n        \"links\":[\\\n          \"https://docs.firecrawl.dev\",\\\n          \"https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete\",\\\n          \"https://docs.firecrawl.dev/api-reference/endpoint/crawl-get\",\\\n          \"https://docs.firecrawl.dev/api-reference/endpoint/crawl-post\",\\\n          \"https://docs.firecrawl.dev/api-reference/endpoint/map\",\\\n          \"https://docs.firecrawl.dev/api-reference/endpoint/scrape\",\\\n          \"https://docs.firecrawl.dev/api-reference/introduction\",\\\n          \"https://docs.firecrawl.dev/articles/search-announcement\",\\\n          ...\\\n        ]\n      }\n    \n\n### \n\n[\u200b](#map-options)\n\nMap Options\n\n#### \n\n[\u200b](#search)\n\n`search`\n\n*   **Type**: `string`\n*   **Description**: Search for links containing specific text.\n*   **Example**: `\"blog\"`\n\n#### \n\n[\u200b](#limit-2)\n\n`limit`\n\n*   **Type**: `integer`\n*   **Description**: Maximum number of links to return.\n*   **Default**: `100`\n\n#### \n\n[\u200b](#ignoresitemap)\n\n`ignoreSitemap`\n\n*   **Type**: `boolean`\n*   **Description**: Ignore the website sitemap when crawling\n*   **Default**: `true`\n\n#### \n\n[\u200b](#includesubdomains)\n\n`includeSubdomains`\n\n*   **Type**: `boolean`\n*   **Description**: Include subdomains of the website\n*   **Default**: `false`\n\nHere is the API Reference for it: [Map Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/map)\n\n[Integrations](/integrations)\n[Scrape](/features/scrape)\n\nOn this page\n\n*   [Basic scraping with Firecrawl (/scrape)](#basic-scraping-with-firecrawl-scrape)\n    \n*   [Scraping PDFs](#scraping-pdfs)\n    \n*   [Scrape Options](#scrape-options)\n    \n*   [Setting the content formats on response with formats](#setting-the-content-formats-on-response-with-formats)\n    \n*   [Getting the full page content as markdown with onlyMainContent](#getting-the-full-page-content-as-markdown-with-onlymaincontent)\n    \n*   [Setting the tags to include with includeTags](#setting-the-tags-to-include-with-includetags)\n    \n*   [Setting the tags to exclude with excludeTags](#setting-the-tags-to-exclude-with-excludetags)\n    \n*   [Waiting for the page to load with waitFor](#waiting-for-the-page-to-load-with-waitfor)\n    \n*   [Setting the maximum timeout](#setting-the-maximum-timeout)\n    \n*   [Example Usage](#example-usage)\n    \n*   [Extractor Options](#extractor-options)\n    \n*   [Using the LLM Extraction](#using-the-llm-extraction)\n    \n*   [schema](#schema)\n    \n*   [system prompt](#system-prompt)\n    \n*   [prompt](#prompt)\n    \n*   [Example Usage](#example-usage-2)\n    \n*   [Actions](#actions)\n    \n*   [Available Actions](#available-actions)\n    \n*   [wait](#wait)\n    \n*   [screenshot](#screenshot)\n    \n*   [click](#click)\n    \n*   [write](#write)\n    \n*   [press](#press)\n    \n*   [scroll](#scroll)\n    \n*   [Crawling Multiple Pages](#crawling-multiple-pages)\n    \n*   [Check Crawl Job](#check-crawl-job)\n    \n*   [Pagination/Next URL](#pagination-next-url)\n    \n*   [Crawler Options](#crawler-options)\n    \n*   [includePaths](#includepaths)\n    \n*   [excludePaths](#excludepaths)\n    \n*   [maxDepth](#maxdepth)\n    \n*   [limit](#limit)\n    \n*   [allowBackwardLinks](#allowbackwardlinks)\n    \n*   [allowExternalLinks](#allowexternallinks)\n    \n*   [scrapeOptions](#scrapeoptions)\n    \n*   [Example Usage](#example-usage-3)\n    \n*   [Mapping Website Links with /map](#mapping-website-links-with-map)\n    \n*   [Usage](#usage)\n    \n*   [Example Response](#example-response)\n    \n*   [Map Options](#map-options)\n    \n*   [search](#search)\n    \n*   [limit](#limit-2)\n    \n*   [ignoreSitemap](#ignoresitemap)\n    \n*   [includeSubdomains](#includesubdomains)", "metadata": {"title": "Advanced Scraping Guide | Firecrawl", "description": "Learn how to improve your Firecrawl scraping with advanced options.", "favicon": "https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/_generated/favicon/favicon-32x32.png?v=3", "language": "en", "ogLocaleAlternate": [], "viewport": "width=device-width", "msapplication-config": "https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/_generated/favicon/browserconfig.xml?v=3", "apple-mobile-web-app-title": "Firecrawl Docs", "application-name": "Firecrawl Docs", "msapplication-TileColor": "#000", "theme-color": "#ffffff", "charset": "utf-8", "og:type": "website", "og:site_name": "Firecrawl Docs", "twitter:card": "summary_large_image", "og:title": "Advanced Scraping Guide | Firecrawl", "twitter:title": "Firecrawl Docs", "og:image": "/images/og.png", "twitter:image": "/images/og.png", "og:description": "Learn how to improve your Firecrawl scraping with advanced options.", "og:url": "/advanced-scraping-guide", "next-head-count": "25", "sourceURL": "https://docs.firecrawl.dev/advanced-scraping-guide", "url": "https://docs.firecrawl.dev/advanced-scraping-guide", "statusCode": 200}}, {"markdown": "[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev)\n\nv1\n\nSearch or ask...\n\nSearch...\n\nNavigation\n\nGet Started\n\nQuickstart\n\n[Documentation](/introduction)\n[SDKs](/sdks/overview)\n[Learn](https://www.firecrawl.dev/blog/category/tutorials)\n[API Reference](/api-reference/introduction)\n\n[\u200b](#welcome-to-firecrawl)\n\nWelcome to Firecrawl\n--------------------------------------------------\n\n[Firecrawl](https://firecrawl.dev?ref=github)\n is an API service that takes a URL, crawls it, and converts it into clean markdown. We crawl all accessible subpages and give you clean markdown for each. No sitemap required.\n\n[\u200b](#how-to-use-it)\n\nHow to use it?\n-------------------------------------\n\nWe provide an easy to use API with our hosted version. You can find the playground and documentation [here](https://firecrawl.dev/playground)\n. You can also self host the backend if you\u2019d like.\n\nCheck out the following resources to get started:\n\n*   [x]  **API**: [Documentation](https://docs.firecrawl.dev/api-reference/introduction)\n    \n*   [x]  **SDKs**: [Python](https://docs.firecrawl.dev/sdks/python)\n    , [Node](https://docs.firecrawl.dev/sdks/node)\n    , [Go](https://docs.firecrawl.dev/sdks/go)\n    , [Rust](https://docs.firecrawl.dev/sdks/rust)\n    \n*   [x]  **LLM Frameworks**: [Langchain (python)](https://python.langchain.com/docs/integrations/document_loaders/firecrawl/)\n    , [Langchain (js)](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/firecrawl)\n    , [Llama Index](https://docs.llamaindex.ai/en/latest/examples/data_connectors/WebPageDemo/#using-firecrawl-reader)\n    , [Crew.ai](https://docs.crewai.com/)\n    , [Composio](https://composio.dev/tools/firecrawl/all)\n    , [PraisonAI](https://docs.praison.ai/firecrawl/)\n    , [Superinterface](https://superinterface.ai/docs/assistants/functions/firecrawl)\n    , [Vectorize](https://docs.vectorize.io/integrations/source-connectors/firecrawl)\n    \n*   [x]  **Low-code Frameworks**: [Dify](https://dify.ai/blog/dify-ai-blog-integrated-with-firecrawl)\n    , [Langflow](https://docs.langflow.org/)\n    , [Flowise AI](https://docs.flowiseai.com/integrations/langchain/document-loaders/firecrawl)\n    , [Cargo](https://docs.getcargo.io/integration/firecrawl)\n    , [Pipedream](https://pipedream.com/apps/firecrawl/)\n    \n*   [x]  **Others**: [Zapier](https://zapier.com/apps/firecrawl/integrations)\n    , [Pabbly Connect](https://www.pabbly.com/connect/integrations/firecrawl/)\n    \n*   [ ]  Want an SDK or Integration? Let us know by opening an issue.\n\n**Self-host:** To self-host refer to guide [here](/contributing/self-host)\n.\n\n### \n\n[\u200b](#api-key)\n\nAPI Key\n\nTo use the API, you need to sign up on [Firecrawl](https://firecrawl.dev)\n and get an API key.\n\n### \n\n[\u200b](#features)\n\nFeatures\n\n*   [**Scrape**](/_sites/docs.firecrawl.dev/introduction#scraping)\n    : scrapes a URL and get its content in LLM-ready format (markdown, structured data via [LLM Extract](/_sites/docs.firecrawl.dev/introduction#extraction)\n    , screenshot, html)\n*   [**Crawl**](/_sites/docs.firecrawl.dev/introduction#crawling)\n    : scrapes all the URLs of a web page and return content in LLM-ready format\n*   [**Map**](/features/map)\n    : input a website and get all the website urls - extremely fast\n\n### \n\n[\u200b](#powerful-capabilities)\n\nPowerful Capabilities\n\n*   **LLM-ready formats**: markdown, structured data, screenshot, HTML, links, metadata\n*   **The hard stuff**: proxies, anti-bot mechanisms, dynamic content (js-rendered), output parsing, orchestration\n*   **Customizability**: exclude tags, crawl behind auth walls with custom headers, max crawl depth, etc\u2026\n*   **Media parsing**: pdfs, docx, images.\n*   **Reliability first**: designed to get the data you need - no matter how hard it is.\n*   **Actions**: click, scroll, input, wait and more before extracting data\n\nYou can find all of Firecrawl\u2019s capabilities and how to use them in our [documentation](https://docs.firecrawl.dev)\n\n[\u200b](#crawling)\n\nCrawling\n--------------------------\n\nUsed to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.\n\n### \n\n[\u200b](#installation)\n\nInstallation\n\n### \n\n[\u200b](#usage)\n\nUsage\n\nIf you\u2019re using cURL or `async crawl` functions on SDKs, this will return an `ID` where you can use to check the status of the crawl.\n\n    {\n      \"success\": true,\n      \"id\": \"123-456-789\",\n      \"url\": \"https://api.firecrawl.dev/v1/crawl/123-456-789\"\n    }\n    \n\n### \n\n[\u200b](#check-crawl-job)\n\nCheck Crawl Job\n\nUsed to check the status of a crawl job and get its result.\n\n#### \n\n[\u200b](#response)\n\nResponse\n\nThe response will be different depending on the status of the crawl. For not completed or large responses exceeding 10MB, a `next` URL parameter is provided. You must request this URL to retrieve the next 10MB of data. If the `next` parameter is absent, it indicates the end of the crawl data.\n\n[\u200b](#scraping)\n\nScraping\n--------------------------\n\nTo scrape a single URL, use the `scrape_url` method. It takes the URL as a parameter and returns the scraped data as a dictionary.\n\n### \n\n[\u200b](#response-2)\n\nResponse\n\nSDKs will return the data object directly. cURL will return the payload exactly as shown below.\n\n    {\n      \"success\": true,\n      \"data\" : {\n        \"markdown\": \"Launch Week I is here! [See our Day 2 Release \ud83d\ude80](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)[\ud83d\udca5 Get 2 months free...\",\\\n        \"html\": \"<!DOCTYPE html><html lang=\\\"en\\\" class=\\\"light\\\" style=\\\"color-scheme: light;\\\"><body class=\\\"__variable_36bd41 __variable_d7dc5d font-inter ...\",\\\n        \"metadata\": {\\\n          \"title\": \"Home - Firecrawl\",\\\n          \"description\": \"Firecrawl crawls and converts any website into clean markdown.\",\\\n          \"language\": \"en\",\\\n          \"keywords\": \"Firecrawl,Markdown,Data,Mendable,Langchain\",\\\n          \"robots\": \"follow, index\",\\\n          \"ogTitle\": \"Firecrawl\",\\\n          \"ogDescription\": \"Turn any website into LLM-ready data.\",\\\n          \"ogUrl\": \"https://www.firecrawl.dev/\",\\\n          \"ogImage\": \"https://www.firecrawl.dev/og.png?123\",\\\n          \"ogLocaleAlternate\": [],\\\n          \"ogSiteName\": \"Firecrawl\",\\\n          \"sourceURL\": \"https://firecrawl.dev\",\\\n          \"statusCode\": 200\\\n        }\\\n      }\\\n    }\\\n    \\\n\\\n[\u200b](#extraction)\\\n\\\nExtraction\\\n------------------------------\\\n\\\nWith LLM extraction, you can easily extract structured data from any URL. We support pydantic schemas to make it easier for you too. Here is how you to use it:\\\n\\\nv1 is only supported on node, python and cURL at this time.\\\n\\\nOutput:\\\n\\\nJSON\\\n\\\n    {\\\n        \"success\": true,\\\n        \"data\": {\\\n          \"extract\": {\\\n            \"company_mission\": \"Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to\",\\\n            \"supports_sso\": true,\\\n            \"is_open_source\": false,\\\n            \"is_in_yc\": true\\\n          },\\\n          \"metadata\": {\\\n            \"title\": \"Mendable\",\\\n            \"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\\\n            \"robots\": \"follow, index\",\\\n            \"ogTitle\": \"Mendable\",\\\n            \"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\\\n            \"ogUrl\": \"https://docs.firecrawl.dev/\",\\\n            \"ogImage\": \"https://docs.firecrawl.dev/mendable_new_og1.png\",\\\n            \"ogLocaleAlternate\": [],\\\n            \"ogSiteName\": \"Mendable\",\\\n            \"sourceURL\": \"https://docs.firecrawl.dev/\"\\\n          },\\\n        }\\\n    }\\\n    \\\n\\\n### \\\n\\\n[\u200b](#extracting-without-schema-new)\\\n\\\nExtracting without schema (New)\\\n\\\nYou can now extract without a schema by just passing a `prompt` to the endpoint. The llm chooses the structure of the data.\\\n\\\nOutput:\\\n\\\nJSON\\\n\\\n    {\\\n        \"success\": true,\\\n        \"data\": {\\\n          \"extract\": {\\\n            \"company_mission\": \"Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to\",\\\n          },\\\n          \"metadata\": {\\\n            \"title\": \"Mendable\",\\\n            \"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\\\n            \"robots\": \"follow, index\",\\\n            \"ogTitle\": \"Mendable\",\\\n            \"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\\\n            \"ogUrl\": \"https://docs.firecrawl.dev/\",\\\n            \"ogImage\": \"https://docs.firecrawl.dev/mendable_new_og1.png\",\\\n            \"ogLocaleAlternate\": [],\\\n            \"ogSiteName\": \"Mendable\",\\\n            \"sourceURL\": \"https://docs.firecrawl.dev/\"\\\n          },\\\n        }\\\n    }\\\n    \\\n\\\n### \\\n\\\n[\u200b](#extraction-v0)\\\n\\\nExtraction (v0)\\\n\\\n[\u200b](#interacting-with-the-page-with-actions)\\\n\\\nInteracting with the page with Actions\\\n--------------------------------------------------------------------------------------\\\n\\\nFirecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.\\\n\\\nHere is an example of how to use actions to navigate to google.com, search for Firecrawl, click on the first result, and take a screenshot.\\\n\\\nIt is important to almost always use the `wait` action before/after executing other actions to give enough time for the page to load.\\\n\\\n### \\\n\\\n[\u200b](#example)\\\n\\\nExample\\\n\\\n### \\\n\\\n[\u200b](#output)\\\n\\\nOutput\\\n\\\n[\u200b](#open-source-vs-cloud)\\\n\\\nOpen Source vs Cloud\\\n--------------------------------------------------\\\n\\\nFirecrawl is open source available under the [AGPL-3.0 license](https://github.com/mendableai/firecrawl/blob/main/LICENSE)\\\n.\\\n\\\nTo deliver the best possible product, we offer a hosted version of Firecrawl alongside our open-source offering. The cloud solution allows us to continuously innovate and maintain a high-quality, sustainable service for all users.\\\n\\\nFirecrawl Cloud is available at [firecrawl.dev](https://firecrawl.dev)\\\n and offers a range of features that are not available in the open source version:\\\n\\\n[\u200b](#contributing)\\\n\\\nContributing\\\n----------------------------------\\\n\\\nWe love contributions! Please read our [contributing guide](https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md)\\\n before submitting a pull request.\\\n\\\n[Launch Week II (New)](/launch-week)\\\n\\\nOn this page\\\n\\\n*   [Welcome to Firecrawl](#welcome-to-firecrawl)\\\n    \\\n*   [How to use it?](#how-to-use-it)\\\n    \\\n*   [API Key](#api-key)\\\n    \\\n*   [Features](#features)\\\n    \\\n*   [Powerful Capabilities](#powerful-capabilities)\\\n    \\\n*   [Crawling](#crawling)\\\n    \\\n*   [Installation](#installation)\\\n    \\\n*   [Usage](#usage)\\\n    \\\n*   [Check Crawl Job](#check-crawl-job)\\\n    \\\n*   [Response](#response)\\\n    \\\n*   [Scraping](#scraping)\\\n    \\\n*   [Response](#response-2)\\\n    \\\n*   [Extraction](#extraction)\\\n    \\\n*   [Extracting without schema (New)](#extracting-without-schema-new)\\\n    \\\n*   [Extraction (v0)](#extraction-v0)\\\n    \\\n*   [Interacting with the page with Actions](#interacting-with-the-page-with-actions)\\\n    \\\n*   [Example](#example)\\\n    \\\n*   [Output](#output)\\\n    \\\n*   [Open Source vs Cloud](#open-source-vs-cloud)\\\n    \\\n*   [Contributing](#contributing)", "metadata": {"title": "Quickstart | Firecrawl", "description": "Firecrawl allows you to turn entire websites into LLM-ready markdown", "favicon": "https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/_generated/favicon/favicon-32x32.png?v=3", "language": "en", "ogLocaleAlternate": [], "viewport": "width=device-width", "msapplication-config": "https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/_generated/favicon/browserconfig.xml?v=3", "apple-mobile-web-app-title": "Firecrawl Docs", "application-name": "Firecrawl Docs", "msapplication-TileColor": "#000", "theme-color": "#ffffff", "charset": "utf-8", "og:type": "website", "og:site_name": "Firecrawl Docs", "twitter:card": "summary_large_image", "og:title": "Quickstart | Firecrawl", "twitter:title": "Firecrawl Docs", "og:image": "/images/og.png", "twitter:image": "/images/og.png", "og:description": "Firecrawl allows you to turn entire websites into LLM-ready markdown", "og:url": "/introduction", "next-head-count": "25", "sourceURL": "https://docs.firecrawl.dev/", "url": "https://docs.firecrawl.dev/introduction", "statusCode": 200}}]}